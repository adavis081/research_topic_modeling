{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ae3ea62-c606-496b-abc0-7ab77a63e70b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# OpenAlex Data Pipeline\n",
    "\n",
    "Author: Alex Davis\n",
    "\n",
    "Date: 07/05/2024\n",
    "\n",
    "The purpose of this script is to ingest OSINT data from the OpenAlex API (https://docs.openalex.org/), preprocess the data, and\n",
    "prepare it for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c1d7feb-eaa0-485e-9be2-79cd0fb7a89e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import packages\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fca2046-1463-41c6-8525-eb806fad5ec6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2eca729-75b0-4b84-b62e-a1dc75f045e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The function below connects to the OpenAlex API and conducts searches based on the parameters (year, keywords, etc.). The function returns a dataframe as a result of the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e489cb3-b244-48be-8c8f-169f625fe6d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def import_data(pages, start_year, end_year, search_terms):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function is used to use the OpenAlex API, conduct a search on works, a return a dataframe with associated works.\n",
    "    \n",
    "    Inputs: \n",
    "        - pages: int, number of pages to loop through\n",
    "        - search_terms: str, keywords to search for (must be formatted according to OpenAlex standards)\n",
    "        - start_year and end_year: int, years to set as a range for filtering works\n",
    "    \"\"\"\n",
    "    \n",
    "    #create an empty dataframe\n",
    "    search_results = pd.DataFrame()\n",
    "    \n",
    "    for page in range(1, pages):\n",
    "        \n",
    "        #use paramters to conduct request and format to a dataframe\n",
    "        response = requests.get(f'https://api.openalex.org/works?page={page}&per-page=200&filter=publication_year:{start_year}-{end_year},type:article&search={search_terms}')\n",
    "        data = pd.DataFrame(response.json()['results'])\n",
    "        \n",
    "        #append to empty dataframe\n",
    "        search_results = pd.concat([search_results, data])\n",
    "    \n",
    "    #subset to relevant features\n",
    "    search_results = search_results[[\"id\", \"title\", \"display_name\", \"publication_year\", \"publication_date\",\n",
    "                                        \"type\", \"countries_distinct_count\",\"institutions_distinct_count\",\n",
    "                                        \"has_fulltext\", \"cited_by_count\", \"keywords\", \"referenced_works_count\", \"abstract_inverted_index\"]]\n",
    "    \n",
    "    return(search_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df649c8d-8845-47f4-97ed-5dc470c39e92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Below, we conduct multiple searches to cover different technology areas inspired by the DoD's critical technology areas (https://www.cto.mil/usdre-strat-vision-critical-tech-areas/). We do this to ensure variety within our data and to have a general idea of the topics included.\n",
    "\n",
    "We set the years from 2016 to 2024 to filter to relevant data. We choose 25 pages to keep the size of each search reasonable. Lastly, we pick a handfull of key terms for each search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3d17445-8afa-4c7b-95c0-7f0565ec316a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#search for Trusted AI and Autonomy\n",
    "ai_search = import_data(35, 2016, 2024, \"'artificial intelligence' OR 'deep learn' OR 'neural net' OR 'autonomous' OR drone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "163f3183-a02c-4d33-a007-05957146807e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#search for Biotechnology\n",
    "biotech_search = import_data(35, 2016, 2024, \"biotech OR dna OR genome OR crispr OR rna\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4739126f-8376-4b2a-98a1-792693413f9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#search for Advanced Materials\n",
    "materials_search = import_data(35, 2016, 2024, \"biomaterial OR 'smart material' OR nanotech OR 'carbon fiber' OR superalloy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4de7f91e-f871-4e6a-9b14-affb07329646",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#search for Space Technology\n",
    "space_search = import_data(35, 2016, 2024, \"satellite OR gps OR 'space navigation' OR 'space communications'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "701ede49-1cb8-40ef-8002-001313adb895",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#search for Human Machine Interfaces\n",
    "interfaces_search = import_data(35, 2016, 2024, \"'augmented reality' OR 'virtual reality' OR 'human-machine interfac' OR 'brain-mach'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9aa60400-79de-4e7c-9509-22d144f3f0ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#concatenate into a master dataframe and drop duplicates/null abstracts\n",
    "master_search = pd.concat([ai_search, biotech_search, materials_search, space_search, interfaces_search])\n",
    "master_search = master_search.drop_duplicates(subset = 'id')\n",
    "master_search = master_search[master_search['abstract_inverted_index'].notna()]\n",
    "\n",
    "print(f\"Final Number of Works: {len(master_search)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "760df7f4-a690-4f1a-ab46-a1fe69fa94d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Preprocess Data\n",
    "\n",
    "The abstracts associated with the works we pulled are returned as an inverted index due to legal reasons. This invereted index can used to return the original text. Then, the text must be cleaned to be prepared for embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f047a38f-b648-4194-b0e7-6d72c15f30c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Inverted Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e669b522-5b7d-45f2-a6d1-1c443ea5b731",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def undo_inverted_index(inverted_index):\n",
    "    \n",
    "    \"\"\"\n",
    "    The purpose of the function is to 'undo' and inverted index. It inputs an inverted index and\n",
    "    returns the original string.\n",
    "    \"\"\"\n",
    "\n",
    "    #create empty lists to store uninverted index\n",
    "    word_index = []\n",
    "    words_unindexed = []\n",
    "    \n",
    "    #loop through index and return key-value pairs\n",
    "    for k,v in inverted_index.items(): \n",
    "        for index in v: word_index.append([k,index])\n",
    "\n",
    "    #sort by the index\n",
    "    word_index = sorted(word_index, key = lambda x : x[1])\n",
    "    \n",
    "    #join only the values and flatten\n",
    "    for pair in word_index:\n",
    "        words_unindexed.append(pair[0])\n",
    "    words_unindexed = ' '.join(words_unindexed)\n",
    "    \n",
    "    return(words_unindexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3edf2e7f-b338-4a04-9129-39e16bb36ea1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create 'original_abstract' feature\n",
    "master_search['original_abstract'] = list(map(undo_inverted_index, master_search['abstract_inverted_index']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4ea26df-0f29-427d-b09f-4da6c00a73dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "master_search.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71e14acc-3a4e-4afc-95ea-56d439cf80d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Combine and Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6177bcd3-5da1-4b56-8b8d-0fd72d3eb62b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#combine all text into one column for analysis and embedding\n",
    "master_search[\"all_text\"] = master_search[\"title\"] + master_search[\"display_name\"] + master_search[\"original_abstract\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2502739f-d8e1-4426-a4db-7d4ce0758ff8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function takes in a string, coverts it to lowercase, cleans\n",
    "    it (remove special character and numbers), and tokenizes it.\n",
    "    \"\"\"\n",
    "    \n",
    "    #convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    #remove special character and digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    #tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36bca763-1729-4134-984b-188ec634df62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function takes in a list of tokens (from the 'preprocess' function) and \n",
    "    removes a list of stopwords. Custom stopwords can be added to the 'custom_stopwords' list.\n",
    "    \"\"\"\n",
    "    \n",
    "    #set default and custom stopwords\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    custom_stopwords = []\n",
    "    stop_words.extend(custom_stopwords)\n",
    "    \n",
    "    #filter out stopwords\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39984313-7f67-48ca-a022-ac991940af15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lemmatize(tokens):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function conducts lemmatization on a list of tokens (from the 'remove_stopwords' function).\n",
    "    This shortens each word down to its root form to improve modeling results.\n",
    "    \"\"\"\n",
    "    \n",
    "    #initalize lemmatizer and lemmatize\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3eecd5d8-eea5-4003-92aa-c3d8f5492315",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function uses the previously defined functions to take a string and\\\n",
    "    run it through the entire data preprocessing process.\n",
    "    \"\"\"\n",
    "    \n",
    "    #clean, tokenize, and lemmatize a string\n",
    "    tokens = preprocess(text)\n",
    "    filtered_tokens = remove_stopwords(tokens)\n",
    "    lemmatized_tokens = lemmatize(filtered_tokens)\n",
    "    clean_text = ' '.join(lemmatized_tokens)\n",
    "    \n",
    "    return(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4062bbc-aac6-41ab-9067-b79f7de91a09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "master_search.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad623277-c184-423d-8010-b47ef380609b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#use functions above to create a column with preprocessed text\n",
    "master_search['clean_text'] = list(map(clean_text, master_search['all_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d48476a3-216d-434c-a6aa-b493e96dd829",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "master_search.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4c9dee5-dee9-44cb-9653-d3101601e2c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Save Data\n",
    "\n",
    "Here, we save the data as a .pkl file. This is a light-weight solution that we can save to the data folder and use in modeling and analysis notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60440f93-a550-4aeb-bfc6-7d7e33e09705",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#save file as .pkl file\n",
    "with open('Data/preprocessed_data.pkl', 'wb') as file: \n",
    "      \n",
    "    # A new file will be created \n",
    "    pickle.dump(master_search, file) "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data_load",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "sagemaker-distribution:Python",
   "language": "python",
   "name": "conda-env-sagemaker-distribution-py"
  },
  "language_info": {
   "name": "python"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
